{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f422ab-9383-48ef-87b1-9ce65a01415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 14:12:31 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/11/07 14:12:31 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-16-78.eu-west-2.compute.internal:4040\n",
      "24/11/07 14:12:31 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/11/07 14:12:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/11/07 14:12:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/11/07 14:12:31 INFO MemoryStore: MemoryStore cleared\n",
      "24/11/07 14:12:31 INFO BlockManager: BlockManager stopped\n",
      "24/11/07 14:12:31 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/11/07 14:12:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/11/07 14:12:31 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/11/07 14:12:32 INFO SparkContext: Running Spark version 3.5.3\n",
      "24/11/07 14:12:32 INFO SparkContext: OS info Linux, 6.1.112-122.189.amzn2023.x86_64, amd64\n",
      "24/11/07 14:12:32 INFO SparkContext: Java version 17.0.13\n",
      "24/11/07 14:12:32 INFO ResourceUtils: ==============================================================\n",
      "24/11/07 14:12:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/11/07 14:12:32 INFO ResourceUtils: ==============================================================\n",
      "24/11/07 14:12:32 INFO SparkContext: Submitted application: YourAppName\n",
      "24/11/07 14:12:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/11/07 14:12:32 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/11/07 14:12:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/11/07 14:12:32 INFO SecurityManager: Changing view acls to: ec2-user\n",
      "24/11/07 14:12:32 INFO SecurityManager: Changing modify acls to: ec2-user\n",
      "24/11/07 14:12:32 INFO SecurityManager: Changing view acls groups to: \n",
      "24/11/07 14:12:32 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/11/07 14:12:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ec2-user; groups with view permissions: EMPTY; users with modify permissions: ec2-user; groups with modify permissions: EMPTY\n",
      "24/11/07 14:12:32 INFO Utils: Successfully started service 'sparkDriver' on port 41155.\n",
      "24/11/07 14:12:32 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/11/07 14:12:32 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/11/07 14:12:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/11/07 14:12:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/11/07 14:12:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/11/07 14:12:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dbd45a4d-8ad2-41de-8229-e430a2bd7465\n",
      "24/11/07 14:12:32 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "24/11/07 14:12:32 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/11/07 14:12:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/11/07 14:12:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/11/07 14:12:32 INFO SparkContext: Added JAR /home/ec2-user/spark-jars/aws-java-sdk-1.11.375.jar at spark://ip-172-31-16-78.eu-west-2.compute.internal:41155/jars/aws-java-sdk-1.11.375.jar with timestamp 1730988752307\n",
      "24/11/07 14:12:32 INFO SparkContext: Added JAR /home/ec2-user/spark-jars/hadoop-aws-3.2.0.jar at spark://ip-172-31-16-78.eu-west-2.compute.internal:41155/jars/hadoop-aws-3.2.0.jar with timestamp 1730988752307\n",
      "24/11/07 14:12:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://18.171.163.114:7077...\n",
      "24/11/07 14:12:32 INFO TransportClientFactory: Successfully created connection to /18.171.163.114:7077 after 3 ms (0 ms spent in bootstraps)\n",
      "24/11/07 14:12:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241107141232-0007\n",
      "24/11/07 14:12:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38511.\n",
      "24/11/07 14:12:32 INFO NettyBlockTransferService: Server created on ip-172-31-16-78.eu-west-2.compute.internal:38511\n",
      "24/11/07 14:12:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/11/07 14:12:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-16-78.eu-west-2.compute.internal, 38511, None)\n",
      "24/11/07 14:12:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-16-78.eu-west-2.compute.internal:38511 with 413.9 MiB RAM, BlockManagerId(driver, ip-172-31-16-78.eu-west-2.compute.internal, 38511, None)\n",
      "24/11/07 14:12:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-16-78.eu-west-2.compute.internal, 38511, None)\n",
      "24/11/07 14:12:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-16-78.eu-west-2.compute.internal, 38511, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 14:12:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#AWS credentials \n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAXQIQAF5NYF4XJHND\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"S6a3sFb5rqZ/rL7Sm7Lc4H3gwix6n4ptTjGBPvR\"\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-2\"\n",
    "\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass  \n",
    "\n",
    "# Start a new Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .master(\"spark://18.171.163.114:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/ec2-user/spark-jars/aws-java-sdk-1.11.375.jar,/home/ec2-user/spark-jars/hadoop-aws-3.2.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set up Hadoop configuration for S3 access \n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.eu-west-2.amazonaws.com\")  # Specify region endpoint if needed\n",
    "\n",
    "# Verify that the Spark session is working\n",
    "print(\"Spark session initialized!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6d2fd8-c577-4cb8-92f9-83363447a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.hadoop.fs.s3a.S3AFileSystem\n"
     ]
    }
   ],
   "source": [
    "#check if haddop dependcy configures it checks but cant find it later ? btw above spark started coeectly \n",
    "print(spark._jsc.hadoopConfiguration().get(\"fs.s3a.impl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2299cc-4675-44e7-9d09-e99d427ec5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 14:14:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/11/07 14:14:17 INFO SharedState: Warehouse path is 'file:/home/ec2-user/spark-warehouse'.\n",
      "24/11/07 14:14:18 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://cloud-project-bucket/movies_metadata.csv.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test loading the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://cloud-project-bucket/movies_metadata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#s3://cloud-project-bucket/movies_metadata.csv\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#https://cloud-project-bucket.s3.eu-west-2.amazonaws.com/movies_metadata.csv\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "#loading the data:here causes an error where connecting to haddop file system been established!\n",
    "df =spark.read.csv(\"s3a://cloud-project-bucket/movies_metadata.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "#s3://cloud-project-bucket/movies_metadata.csv\n",
    "#https://cloud-project-bucket.s3.eu-west-2.amazonaws.com/movies_metadata.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80fcc9-417c-444b-9e8f-d8e7a45af114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from phase 2 proccesd the same the diffreince only in connecting the bucket,now this is the master node it should compute on its 2 connected slaves\n",
    "\n",
    "#the columns im intrested in ,my current data frame\n",
    "selected_columns=df.select(\"id\",\"budget\",\"vote_average\")\n",
    "print(selected_columns)\n",
    "#by default, Spark treats the JSON data in the column as a string,so ww will have to deal with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd443d-babe-496f-af79-7b15ae931fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim,col\n",
    "#handleing casting with white space since its a string so it wont be null after casting\n",
    "\n",
    "selected_columns = selected_columns.withColumn(\"budget\", trim(col(\"budget\")))\n",
    "selected_columns = selected_columns.withColumn(\"vote_average\", trim(col(\"vote_average\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c6594-46e7-41d3-8ebd-da8f518e4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this section ill clean all columns but the json for later\n",
    "\n",
    "##col:function is used to refer to a DataFrame column by name. It allows you to perform various operation,like casting a col\n",
    "selected_columns = selected_columns.withColumn(\"budget\", col(\"budget\").cast(\"float\"))\n",
    "selected_columns = selected_columns.withColumn(\"vote_average\", col(\"vote_average\").cast(\"float\"))\n",
    "selected_columns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b8b5-7e5a-4dac-beb4-8c30d9c1f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Missing Values [null]\n",
    "\n",
    "print(\"Before fixing null values:\")\n",
    "selected_columns.filter(\n",
    "    selected_columns.id.isNull() | \n",
    "    selected_columns.budget.isNull() | \n",
    "    selected_columns.vote_average.isNull()\n",
    ").show()\n",
    "\n",
    "# Drop rows where either 'budget' or 'vote_average' is null\n",
    "# Since both are critical to my analysis\n",
    "selected_columns = selected_columns.dropna(subset=[\"budget\", \"vote_average\"])\n",
    "\n",
    "# Check for Missing Values [null] after fixing\n",
    "print(\"After fixing null values:\")\n",
    "selected_columns.filter(\n",
    "    selected_columns.id.isNull() | \n",
    "    selected_columns.budget.isNull() | \n",
    "    selected_columns.vote_average.isNull() \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676515a-5296-44f7-b71c-7c4745c398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates rows ill measure by id, method of pyspark takes a sub set[] inside the col name \n",
    "selected_columns = selected_columns.dropDuplicates([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd283e2d-bde9-4a56-8ec6-8bda150654f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim whitespace \n",
    "selected_columns = selected_columns.withColumn(\"id\", trim(col(\"id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333f8cc-5ab5-4121-8a77-8cdc8e3cc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why are all budget showing 0?\n",
    "# Remove rows with a budget of 0\n",
    "selected_columns = selected_columns.filter(selected_columns.budget > 0)#are we removing them\n",
    "# Check remaining rows after filtering\n",
    "print(\"Remaining rows after removing zero budget:\")\n",
    "selected_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa00a18-978f-45a6-8024-7587c70597b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # ml only can take vector so \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator #evalute ur modle\n",
    "#ml configration i wanna implment linear reggersion,no need for id\n",
    "selected_columns = selected_columns.drop(\"id\")\n",
    "selected_columns.show()\n",
    "#budget is my feature[input] my label is vote[the dependent variable]\n",
    "\n",
    "#ml expect input as a form of vector\n",
    "\n",
    "# Step 1: Create a feature vector for budget ,creating a new column called features\n",
    "assembler = VectorAssembler(inputCols=[\"budget\"], outputCol=\"features\")\n",
    "assembled_data = assembler.transform(selected_columns)\n",
    "\n",
    "# Step 2: Split the data into training 80% and test 20% \n",
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Step 3: Set up the linear regression model get my selected columns for predictions\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"vote_average\")\n",
    "\n",
    "# Step 4: Train the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Step 5: Output the model's coefficients and intercept\n",
    "#coefficients and intercept is a crucial part of understanding how the linear regression model predicts outcomes y=mx+b\n",
    "# generalization based on the relationships identified across all of your training data\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "\n",
    "# Step 6: Make predictions on the test data the 20%\n",
    "predictions = lr_model.transform(test_data)\n",
    "predictions.select(\"budget\", \"vote_average\", \"prediction\").show()\n",
    "\n",
    "# Step 7: Evaluate the model using RMSE (Root Mean Squared Error) this is the standard for linear reggrssion\n",
    "evaluator = RegressionEvaluator(labelCol=\"vote_average\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ff7f-e22d-46d1-98a2-80d150e52a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract the columns for budget, actual vote_average, and predicted vote_average and turn them to pandas data frame\n",
    "x = predictions.select(\"budget\").toPandas()\n",
    "y_actual = predictions.select(\"vote_average\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "# Create a scatter plot of actual vote_average vs budget\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=x['budget'], y=y_actual['vote_average'], label=\"Actual\", alpha=0.6, color='blue')\n",
    "\n",
    "\n",
    "sns.lineplot(x=x['budget'], y=y_pred['prediction'], label=\"Predicted\", color='red')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title(\"Actual vs Predicted Vote Average vs. Budget\")\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Vote Average\")\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce652e1-36e2-45fa-b16d-31b4e9d8e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second file \n",
    "#step 1\n",
    "# Step 2: Load  data\n",
    "\n",
    "df_Bclean=spark.read.csv(\"s3://cloud-project-bucket/ratings.csv\", header=True, inferSchema=True)\n",
    "# Step 3: Show the full DataFrame schema and first few rows (optional)\n",
    "df_Bclean. printSchema\n",
    "df_Bclean. show(5)\n",
    "# Step 4: Check the total number of records\n",
    "total_records = df_Bclean.count ()\n",
    "print(f\"Total number of records in the dataset: {total_records}\")\n",
    "\n",
    "# Step 5: Extract a random sample\n",
    "# Using sample with fraction argument to approximate 10 million rows if you know the total number of rows\n",
    "fraction = 10_000_000 / total_records # Calculate the fraction needed to extract 10 million rows\n",
    "df_sample = df_Bclean.sample(withReplacement=False, fraction=fraction)\n",
    "\n",
    "# the number of records in the sampled dataset\n",
    "sample_count = df_sample. count()\n",
    "print(f\"Number of records in the sampled dataset: {sample_count}\")\n",
    "\n",
    "# Step 6: get a random sample 10 mill\n",
    "df_sample_10M = df_sample. limit (10_000_000)\n",
    "# Step 7: Select the columns (movieId, userId, rating)\n",
    "df_selected = df_sample.select(\"movieId\", \"userId\", \"rating\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ce7e5-4d39-4f2d-a3a8-8709628dd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting\n",
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# Trim leading/trailing spaces and cascade the conversion from String to Integer for userId and movieId\n",
    "df_casting = df_selected.withColumn(\"userId\", trim(col(\"userId\")).cast(\"int\")) \\\n",
    "                        .withColumn(\"movieId\", trim(col(\"movieId\")).cast(\"int\"))\n",
    "\n",
    "# Display the schema after conversion\n",
    "print(\"Schema after casting to Integer:\")\n",
    "df_casting.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import col, trim, sum, when  # Import col and trim functions\n",
    "\n",
    "# Step 10: Data Cleaning\n",
    "# Check for missing values in each column\n",
    "missing_values = df_casting.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_casting.columns])\n",
    "print(\"Missing Values Count per Column:\")\n",
    "missing_values.show()\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_cleaned = df_casting.dropna()\n",
    "\n",
    "# Step 3: Verify the cleaned DataFrame\n",
    "print(\"Number of records after dropping missing values:\")\n",
    "print(df_cleaned.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162233c7-f9b4-4991-ae46-8d27d8d6b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Records Where movieId is 0 or Null\n",
    "df_cleaned = df_cleaned.filter((col(\"movieId\").isNotNull()) & (col(\"movieId\") != 0))\n",
    "\n",
    "\n",
    "df_cleaned = df_cleaned.filter((col(\"userId\").isNotNull()) & (col(\"userId\") != 0))\n",
    "\n",
    "#  Allow rating to be 0, but Remove Null Ratings\n",
    "df_cleaned = df_cleaned.filter(col(\"rating\").isNotNull())\n",
    "\n",
    "# 1 Check for Duplicate Records and Remove Duplicates (Corrected column name: \"movieId\")\n",
    "df_cleaned = df_cleaned.dropDuplicates([\"userId\", \"movieId\", \"rating\"])\n",
    "\n",
    "# 1Rating Range Validation (0 to 10)\n",
    "df_cleaned = df_cleaned.filter((col(\"rating\") >= 0) & (col(\"rating\") <= 10))\n",
    "\n",
    "# Display the cleaned data\n",
    "df_cleaned.show()\n",
    "\n",
    "from pyspark.sql.functions import avg, count, col\n",
    "\n",
    "# Feature 1: Count the number of ratings for each movie\n",
    "movie_rating_count = df_cleaned.groupBy(\"movieId\").agg(count(\"rating\").alias(\"rating_count\"))\n",
    "\n",
    "# Feature 2: Calculate the average rating for each movie\n",
    "movie_avg_rating = df_cleaned.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# Feature 3: Count the number of unique users per movie\n",
    "unique_users = df_cleaned.groupBy(\"movieId\").agg(count(\"userId\").alias(\"user_count\"))\n",
    "\n",
    "# Join the features together into a single DataFrame\n",
    "features_df = movie_rating_count.join(movie_avg_rating, on=\"movieId\").join(unique_users, on=\"movieId\")\n",
    "\n",
    "# Show the feature DataFrame\n",
    "features_df.show(20)\n",
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "# Step 3: Aggregate data by movieId\n",
    "movie_stats = df_cleaned.groupBy(\"movieId\").agg(\n",
    "    count(\"rating\").alias(\"ratings_count\"),  # Count the number of ratings for each movie\n",
    "    avg(\"rating\").alias(\"avg_rating\")        # Average rating for each movie\n",
    ")\n",
    "\n",
    "# Show aggregated data and inspect before model\n",
    "movie_stats.show(5)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 4: Create a label column\n",
    "# Label = 1 if movie has more than 1000 ratings, else 0\n",
    "movie_stats = movie_stats.withColumn(\"label\", F.when(movie_stats[\"ratings_count\"] > 1000, 1).otherwise(0))\n",
    "\n",
    "# Show data with labels\n",
    "movie_stats.show(5)\n",
    "\n",
    "# Step 5: Separate movies into \"high\" and \"low\" groups based on the label\n",
    "high_movies = movie_stats.filter(movie_stats[\"label\"] == 1)\n",
    "low_movies = movie_stats.filter(movie_stats[\"label\"] == 0)\n",
    "\n",
    "# Step 6: Find the movie with the highest rating count in the \"high\" group\n",
    "highest_rated_high = high_movies.orderBy(F.desc(\"ratings_count\")).limit(1)\n",
    "\n",
    "# Step 7: Find the movie with the lowest rating count in the \"low\" group\n",
    "lowest_rated_low = low_movies.orderBy(F.asc(\"ratings_count\")).limit(1)\n",
    "\n",
    "# Show the highest and lowest movies from each group\n",
    "print(\"Highest Rated Movie from High Group:\")\n",
    "highest_rated_high.show()\n",
    "high_movies.show(5)\n",
    "\n",
    "print(\"Lowest Rated Movie from Low Group:\")\n",
    "lowest_rated_low.show()\n",
    "\n",
    "low_movies.show(5)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create the features vector\n",
    "assembler = VectorAssembler(inputCols=[\"ratings_count\", \"avg_rating\"], outputCol=\"features\")\n",
    "ml_data = assembler.transform(movie_stats)\n",
    "\n",
    "# Split data into train/test sets\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "#Display predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
    "# Evaluate the model using ROC-AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC-AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f5e5f-c443-4f26-951f-15177ebdd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "movie_stats = movie_stats.withColumn('label', when(movie_stats['ratings_count'] > 1000, 1).otherwise(0))\n",
    "\n",
    "\n",
    "data_pandas = movie_stats.select(\"movieId\", \"ratings_count\", \"label\").toPandas()\n",
    "\n",
    "\n",
    "high_rated_movies = data_pandas[data_pandas['label'] == 1]\n",
    "low_rated_movies = data_pandas[data_pandas['label'] == 0]\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Plot low-rated movies with blue dots\n",
    "plt.scatter(low_rated_movies['movieId'], low_rated_movies['ratings_count'], color='blue', label='Low Rated (<1000 ratings)', alpha=0.7)\n",
    "\n",
    "# Plot high-rated movies with red dots\n",
    "plt.scatter(high_rated_movies['movieId'], high_rated_movies['ratings_count'], color='red', label='High Rated (>1000 ratings)', alpha=0.7)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Scatter Plot of Movie Ratings Frequency')\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('Rating Count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
